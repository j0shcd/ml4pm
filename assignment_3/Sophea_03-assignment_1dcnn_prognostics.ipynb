{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89q7mbPv9NjU"
   },
   "source": [
    "# Prognostics for Turbofine Engines with 1D Convolutional Neural Networks ü©∫\n",
    "\n",
    "Prognostics is the prediction of Remaining useful life of instance of failure of a component based on the knowledge about current and future coditions of operation (obtained through various sensors or physical models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFd6spkDEvi4"
   },
   "source": [
    "The new C-MAPSS dataset DS02 from NASA provides degradation trajectories of 9 turbofan engines with unknown and different initial health condition for complete flights and two failure modes (HPT efficiency degradation & HPT efficiency degradation combined with LPT efficiency and capacity degradation). The data were synthetically generated with the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dynamical model. The data contains multivariate sensors readings of the complete run-to-failure trajectories. Therefore, the records stop at the cycle/time the engine failed. A total number of 6.5M time stamps are available. Dataset copyright (c) by Manuel Arias.\n",
    "\n",
    "**For training simplicity, the dataset has been preprocessed. The dataset has been downsampled from 1Hz to 0.1 Hz with an IIR 8th Order Chebyshev filter. Data format has been converted from double to float precison.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3QI5TCpSmU0"
   },
   "source": [
    "## Imports üíº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CVNWlBJ9Ija"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdxpcrt_SqI4"
   },
   "source": [
    "## Download dataset üîó\n",
    "\n",
    "Download the dataset from [Google Drive Link](https://drive.google.com/file/d/1jXC3BQmEkupXkJbuXM52-Lf6ADLzR4Rr/view?usp=sharing) and put it in the folder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ccLNkIa9Xvx",
    "outputId": "1dd1e3d4-4587-46f1-871e-f72159be57c3"
   },
   "outputs": [],
   "source": [
    "folder = os.getcwd()\n",
    "filename = f'{folder}/ncmapps_ds02.csv'\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3y1SgKPqSwq"
   },
   "source": [
    "# Data exploration üîç\n",
    "\n",
    "<img src=\"images/cmapss.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0pa75ICAUU1"
   },
   "source": [
    "### Feature descriptions\n",
    "\n",
    "| Symbol |           Description           | Units |\n",
    "|:------:|:-------------------------------:|:-----:|\n",
    "|   Wf   |            Fuel flow            |  pps  |\n",
    "|   Nf   |        Physical fan speed       |  rpm  |\n",
    "|   Nc   |       Physical core speed       |  rpm  |\n",
    "|   T24  | Total temperature at LPC outlet |   ¬∞R  |\n",
    "|   T30  | Total temperature at HPC outlet |   ¬∞R  |\n",
    "|   T48  | Total temperature at HPT outlet |   ¬∞R  |\n",
    "|   T50  | Total temperature at LPT outlet |   ¬∞R  |\n",
    "|   P15  |  Total pressure in bypass-duct  |  psia |\n",
    "|   P2   |   Total pressure at fan inlet   |  psia |\n",
    "|   P21  |   Total pressure at fan outlet  |  psia |\n",
    "|   P24  |   Total pressure at LPC outlet  |  psia |\n",
    "|  Ps30  |  Static pressure at HPC outlet  |  psia |\n",
    "|   P40  | Total pressure at burner outlet |  psia |\n",
    "|   P50  |   Total pressure at LPT outlet  |  psia |\n",
    "|   alt  |             Altitude            |   ft  |\n",
    "|  Mach  |        Flight Mach number       |   -   |\n",
    "|   TRA  |     Throttle‚Äìresolver angle     |   %   |\n",
    "|   T2   |  Total temperature at fan inlet |   ¬∞R  |\n",
    "|  cycle |       Flight cycle number       |   -   |\n",
    "|   Fc   |           Flight class          |   -   |\n",
    "|   hs   |           Health state          |   -   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "Au1CJA3b95-f",
    "outputId": "b873d351-3e30-4240-d04e-c24c4ffc9520"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "0pxG0gHm_Jc9",
    "outputId": "0cdca513-b6e5-403c-91a0-8c99c83d8159"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5hBbUbqCIMB"
   },
   "source": [
    "### Flight Classes\n",
    "\n",
    "The units are divided into three flight classes depending on whether the unit is operating short-length flights (i.e., flight class 1), medium-length flights (i.e., flight class 2), or long-length flights (i.e., flight class 2). A number of real flight conditions are available within each of the flight classes.\n",
    "\n",
    "| Flight Class   | Flight Length [h]\n",
    "| :-----------:  | :-----------:    \n",
    "| 1              |    1 to 3        \n",
    "| 2              |    3 to 5        \n",
    "| 3              |    5 to 7        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "BY-bP6gMoLSn",
    "outputId": "c434b014-d132-489b-ffc0-0f74f5c413a5"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(\n",
    "    subset=['unit','Fc'], keep='last'\n",
    "    ).plot(\n",
    "        x='unit', y='Fc', \n",
    "        kind='bar', \n",
    "        xlabel='Unit # [-]', \n",
    "        ylabel='Flight Class # [-]'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVxR_2YcsAun"
   },
   "source": [
    "## Feature Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-WToY0EQ8gJ"
   },
   "outputs": [],
   "source": [
    "LABELS = ['RUL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQUvTSv8GSeE"
   },
   "source": [
    "Operative Conditions ($w$)\n",
    "\n",
    "DASHlink- Flight Data For Tail 687.(2012). Retrieved on 2019-01-29 from https://c3.nasa.gov/dashlink/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrHh3rYqGhGt"
   },
   "outputs": [],
   "source": [
    "W_VAR = ['alt', 'Mach', 'TRA', 'T2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQvRwIJqGN7H"
   },
   "source": [
    "Sensor readings ($X_s$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjgyPJPNGiOI"
   },
   "outputs": [],
   "source": [
    "XS_VAR = ['T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "k_7o8zlKGmWq",
    "outputId": "b2b28b40-09ad-4249-cb13-b6bad0ca5978"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "sns.heatmap(df[W_VAR+XS_VAR+LABELS].corr(), cmap=\"vlag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJIwiz_LFDHM"
   },
   "source": [
    "### Flight Traces\n",
    "visualize a single flight trace of a given unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "42WQNKPuo5On",
    "outputId": "41deb687-571c-499a-cfe8-44b4757ec517"
   },
   "outputs": [],
   "source": [
    "unit = 14\n",
    "cycle = 70\n",
    "df_u_sel = df.loc[(df.unit == unit) & (df.cycle == cycle)][W_VAR+XS_VAR]\n",
    "df_u_sel.reset_index(inplace=True, drop=True)\n",
    "axes = df_u_sel.plot(figsize=(12, 10), subplots=True, layout=(5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AYnARDgvcir"
   },
   "source": [
    "### Flight envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "F8YH_UgKvl-B",
    "outputId": "fbe278b5-5bc6-468a-9f50-92be11b2b150"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1)\n",
    "for i in [3, 2, 1]:\n",
    "    df.loc[df['Fc'] == i].plot(x='Mach', y='alt', alpha=0.8, label=f'Fc={i}', ax=ax, lw=5)\n",
    "plt.xlabel('Mach Number - [-]')\n",
    "plt.ylabel('Flight Altitude - [ft]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFMiKdPj2Tam"
   },
   "source": [
    "## Histogram of Flight Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "id": "KjkHl4fNyrMz",
    "outputId": "45936d24-c78b-4599-b72b-e0d442f23763"
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for i, var in enumerate(W_VAR):\n",
    "    sns.kdeplot(data=df[W_VAR+['unit']], x=var, hue='unit', shade=True, gridsize=100, ax=axes[i//2][i%2], palette=sns.color_palette(\"husl\", 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFJVdOWqqekG"
   },
   "source": [
    "# Developing your first prognostics model üíª\n",
    "Step by step, you will learn how to build your first prognostics model from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNBXLCmgFImE"
   },
   "source": [
    "## Define Sequence Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVFFWkhwFHP3"
   },
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, dataframe, window=50, stride=1, horizon=1, device='cpu'):\n",
    "        \"\"\"Sliding window dataset with RUL label\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): dataframe containing scenario descriptors and sensor reading\n",
    "            window (int, optional): sequence window length. Defaults to 50.\n",
    "            stride (int, optional): data stride length. Defaults to 1.\n",
    "            horizon (int, optional): prediction forcasting length. Defaults to 1.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.stride = stride\n",
    "        self.horizon = horizon\n",
    "        \n",
    "        self.X = np.array(dataframe[XS_VAR+W_VAR].values).astype(np.float32)\n",
    "        self.y = np.array(dataframe['RUL'].values).astype(np.float32)\n",
    "        if 'ds' in dataframe.columns:\n",
    "            unqiue_cycles = dataframe[['ds', 'unit', 'cycle']].value_counts(sort=False)\n",
    "        else:\n",
    "            unqiue_cycles = dataframe[['unit', 'cycle']].value_counts(sort=False)\n",
    "        self.indices = torch.from_numpy(self._get_indices(unqiue_cycles)).to(device)\n",
    "\n",
    "    # TODO add comment\n",
    "    def _get_indices(self, unqiue_cycles):\n",
    "        cycles = unqiue_cycles.to_numpy()\n",
    "        idx_list = []\n",
    "        for i, c_count in enumerate(cycles):\n",
    "            c_start = sum(cycles[:i])\n",
    "            c_end = c_start + (c_count - self.window - self.horizon)\n",
    "            if c_end + self.horizon < len(self.X): # handling y not in the last seq case\n",
    "                idx_list += [_ for _ in np.arange(c_start, c_end + 1, self.stride)]\n",
    "        return np.asarray([(idx, idx+self.window) for idx in idx_list])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        i_start, i_stop = self.indices[i]\n",
    "        x = self.X[i_start:i_stop, :]\n",
    "        y = self.y[i_start]\n",
    "        x = x.permute(1, 0)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8_R6aEIG2Iy"
   },
   "outputs": [],
   "source": [
    "def create_datasets(df, window_size, train_units, test_units, device='cpu'):\n",
    "    df_train = df[df['unit'].isin(train_units)]\n",
    "    train_dataset = SlidingWindowDataset(df_train, window=window_size)\n",
    "\n",
    "    df_test = df[df['unit'].isin(test_units)]    \n",
    "    test_dataset = SlidingWindowDataset(df_test, window=window_size)\n",
    "\n",
    "    # normalizing features\n",
    "    scaler = MinMaxScaler()\n",
    "    train_dataset.X = scaler.fit_transform(train_dataset.X)\n",
    "    test_dataset.X = scaler.transform(test_dataset.X)\n",
    "\n",
    "    # convert numpy array to tensors\n",
    "    datasets = [train_dataset, test_dataset]\n",
    "    for d in datasets:\n",
    "        d.X = torch.from_numpy(d.X).to(device)\n",
    "        d.y = torch.from_numpy(d.y).to(device)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def create_data_loaders(datasets, batch_size=256, val_split=0.2):\n",
    "    # fixed seed for data splits for reproducibility\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    d_train, d_test = datasets\n",
    "    dataset_size = len(d_train)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(val_split * dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(d_train, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader = DataLoader(d_train, batch_size=batch_size, sampler=valid_sampler)\n",
    "    test_loader = DataLoader(d_test, batch_size=batch_size, shuffle=False)      \n",
    "\n",
    "    d_info = f\"train_size: {len(train_indices)}\\t\"\n",
    "    d_info += f\"validation_size: {len(val_indices)}\\t\"\n",
    "    d_info += f\"test_size: {len(d_test)}\"\n",
    "    print(d_info)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iuOuWQYuHlw"
   },
   "source": [
    "## Define Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQ6S9PhqOA_G"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        n_epochs=20,\n",
    "        criterion=nn.MSELoss(),\n",
    "        model_name='best_model',\n",
    "        seed=42,\n",
    "        device='cpu'\n",
    "    ):\n",
    "        self.seed = seed\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.n_epochs = n_epochs\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        # adding time_stamp to model name to make sure the save models don't overwrite each other, \n",
    "        # you can customize your own model name with hyperparameters so that you can reload the model more easily\n",
    "        time_stamp = time.strftime(\"%m%d%H%M%S\")\n",
    "        self.model_path = f'models/{model_name}_{time_stamp}.pt'\n",
    "\n",
    "        self.losses = {split: [] for split in ['train', 'eval', 'test']}\n",
    "        \n",
    "    def compute_loss(self, x, y, model=None):\n",
    "        y = y.view(-1)\n",
    "        y_pred = self.model(x)\n",
    "        y_pred = y_pred.view(-1)\n",
    "        loss = self.criterion(y, y_pred)\n",
    "        return loss, y_pred, y\n",
    "    \n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        # batch losses\n",
    "        b_losses = []\n",
    "        for x, y in loader:\n",
    "            # Setting the optimizer gradient to Zero\n",
    "            self.optimizer.zero_grad()\n",
    "            x.to(torch.device(self.device))\n",
    "            y.to(torch.device(self.device))\n",
    "            \n",
    "            loss, pred, target = self.compute_loss(x, y)\n",
    "            \n",
    "            # Backpropagate the training loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            b_losses.append(loss.detach().numpy())\n",
    "        \n",
    "        # aggregated losses across batches\n",
    "        agg_loss = np.sqrt((np.asarray(b_losses) ** 2).mean())\n",
    "        self.losses['train'].append(agg_loss)\n",
    "        return agg_loss\n",
    "\n",
    "    # decorator, equivalent to with torch.no_grad():\n",
    "    @torch.no_grad()\n",
    "    def eval_epoch(self, loader, split='eval'):\n",
    "        self.model.eval()\n",
    "        # batch losses\n",
    "        b_losses = []\n",
    "        for x, y in loader:\n",
    "            x.to(torch.device(self.device))\n",
    "            y.to(torch.device(self.device))\n",
    "            \n",
    "            loss, pred, target = self.compute_loss(x, y)\n",
    "            \n",
    "            b_losses.append(loss.detach().numpy())\n",
    "        \n",
    "        # aggregated losses across batches\n",
    "        agg_loss = np.sqrt((np.asarray(b_losses) ** 2).mean())\n",
    "        self.losses[split].append(agg_loss)\n",
    "        return agg_loss\n",
    "        \n",
    "    def fit(self, loaders):\n",
    "        print(f\"Training model for {self.n_epochs} epochs...\")\n",
    "        train_loader, eval_loader, test_loader = loaders\n",
    "        train_start = time.time()\n",
    "        \n",
    "        start_epoch = 0\n",
    "        best_eval_loss = np.inf\n",
    "            \n",
    "        for epoch in range(start_epoch, self.n_epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            eval_loss = self.eval_epoch(eval_loader, split='eval')\n",
    "            test_loss = self.eval_epoch(test_loader, split='test')\n",
    "\n",
    "            if eval_loss < best_eval_loss:\n",
    "                best_eval_loss = eval_loss\n",
    "                self.save(self.model, self.model_path)\n",
    "\n",
    "            s = (\n",
    "                f\"[Epoch {epoch + 1}] \"\n",
    "                f\"train_loss = {train_loss:.5f}, \"\n",
    "                f\"eval_loss = {eval_loss:.5f}, \"\n",
    "                f\"test_loss = {test_loss:.5f}\"\n",
    "            )\n",
    "\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            s += f\" [{epoch_time:.1f}s]\"\n",
    "            print(s)\n",
    "    \n",
    "        train_time = int(time.time() - train_start)\n",
    "                \n",
    "        print(f'Task done in {train_time}s')\n",
    "    \n",
    "    @ staticmethod\n",
    "    def evaluate_model_performance(y_pred, y_true):\n",
    "        \"\"\"\n",
    "        ## Task: Define Model Performance on RMSE and NASA score\n",
    "        The performance of your implemented model should be evaluated using two common metrics applied in N-CMAPSS prognostics analysis:\n",
    "        RMSE and NASA-score (in 1E5) as introduced in [\"Fusing Physics-based and Deep Learning Models for Prognostics\"](https://arxiv.org/abs/2003.00732)\n",
    "        \"\"\"\n",
    "        # TODO: add implementation\n",
    "        score = ...\n",
    "        score = score / 1e5\n",
    "        rmse = ...\n",
    "        return score, rmse\n",
    "    \n",
    "    # decorator, equivalent to with torch.no_grad():\n",
    "    @torch.no_grad()\n",
    "    def eval_rul_prediction(self, test_loader):\n",
    "        print(f\"Evaluating test RUL...\")\n",
    "        \n",
    "        ## MASK OUT EVAL and add explanation\n",
    "        best_model = self.load(self.model) \n",
    "        best_model.eval()\n",
    "        \n",
    "        preds = []\n",
    "        trues = []\n",
    "        \n",
    "        for x, y in tqdm(test_loader):\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            _, y_pred, y_target = self.compute_loss(x, y)\n",
    "            preds.append(y_pred.detach().cpu().numpy())\n",
    "            trues.append(y_target.detach().cpu().numpy())\n",
    "        \n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        trues = np.concatenate(trues, axis=0)\n",
    "        \n",
    "        df = pd.DataFrame({         \n",
    "            'pred': preds,\n",
    "            'true': trues,\n",
    "            'err': np.sqrt((preds - trues)**2)\n",
    "        })\n",
    "        \n",
    "        score, rmse = self.evaluate_model_performance(preds, trues)\n",
    "        df_out = pd.DataFrame({\n",
    "            'score': [score],\n",
    "            'rmse': [rmse],\n",
    "            'seed': [self.seed],\n",
    "        })\n",
    "        return df, df_out\n",
    "\n",
    "    def save(self, model, model_path=None):\n",
    "        os.makedirs(f'{folder}/models', exist_ok=True)\n",
    "        if model_path is None:\n",
    "            model_path = self.model_path \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "    def load(self, model, model_path=None):\n",
    "        \"\"\"\n",
    "        loads the prediction model's parameters\n",
    "        \"\"\"\n",
    "        if model_path is None:\n",
    "            model_path = self.model_path\n",
    "        model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        print(f\"Model {model.__class__.__name__} saved in {model_path} loaded to {self.device}\")\n",
    "        return model\n",
    "\n",
    "    def plot_losses(self):\n",
    "        \"\"\"\n",
    "        :param losses: dict with losses\n",
    "        \"\"\"\n",
    "        linestyles = {\n",
    "            'train': 'solid', \n",
    "            'eval': 'dashed', \n",
    "            'test': 'dotted', \n",
    "        }\n",
    "        for split, loss in self.losses.items():\n",
    "            ls = linestyles[split]\n",
    "            plt.plot(range(1, 1+len(loss)), loss, label=f'{split} loss', linestyle=ls)\n",
    "            plt.yscale('log')\n",
    "                \n",
    "        plt.title(\"Training/Validation Losses\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igknd0rYN56n"
   },
   "source": [
    "# Task: Implement a 1D Convolutional Neural Network (1DCNN) Model\n",
    "Conventional CNN's developed for image tasks learn to extract features from the 2D input data. They are autonomous (require no domain expertise or prior info about the image) and thus can be applied to any image regardless of its dimensions. This is due to the fact that these CNN's go through an image by downsampling the image which we call straddling or windowing.  \n",
    "\n",
    "Similarly 1D CNN learns to extract features from a time series data, by windowing over the data, considering a set of data observations each time. The benefit of using the CNN for sequence classification is that it can learn from the raw time series data, and in turn do not require domain expertise to engineer relevant features.\n",
    "\n",
    "The CNN architecture outlined in the paper consists of five layers. The initial three layers are convolutional layers, each employing filters of size 10. The first two convolutional layers consist of ten channels each, while the third convolutional layer comprises a single channel. Zero padding is utilized to maintain the dimensions of the feature map throughout the network. Following the convolutional layers, the 2D feature map is flattened, leading into a 50-unit fully connected layer, and subsequently, a linear output neuron. The activation function used across the network is ReLU. The network encompasses 24,000 trainable parameters.\n",
    "\n",
    "As an advanced extension of this task, you are encouraged to explore the inclusion of Dropout and Batch Normalization layers as regularization techniques to improve the model's generalization performance. Implementing these additional layers can help in reducing overfitting and ensuring that the model generalizes well to unseen data. Your exploration should evaluate the impact of these regularization techniques on the model's performance and compare the results with the baseline model (without Dropout and Batch Normalization).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/assignment_1dcnn.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K831VUjPLSlC"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.BatchNorm1d):\n",
    "        m.weight.data.fill_(1.0)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "        m.weight.data = nn.init.xavier_uniform_(\n",
    "            m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    A 1D-CNN model that is based on the paper \"Fusing physics-based and deep learning models for prognostics\"\n",
    "    from Manuel Arias Chao et al. (with batchnorm layers)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels=18, \n",
    "                 out_channels=1,\n",
    "                 window=50, \n",
    "                 n_ch=10, \n",
    "                 n_k=10, \n",
    "                 n_hidden=50, \n",
    "                 n_layers=3,\n",
    "                 dropout=0.,\n",
    "                 padding='same'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_features (int, optional): number of input features. Defaults to 18.\n",
    "            window (int, optional): sequence length. Defaults to 50.\n",
    "            n_ch (int, optional): number of channels (filter size). Defaults to 10.\n",
    "            n_k (int, optional): kernel size. Defaults to 10.\n",
    "            n_hidden (int, optional): number of hidden neurons for regressor. Defaults to 50.\n",
    "            n_layers (int, optional): number of convolution layers. Defaults to 5.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: implement model architecture\n",
    "\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: implement forward pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-JGs7gqhn8w"
   },
   "source": [
    "# Training a model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84R7i9gF4KFD"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    r\"\"\"Sets the seed for generating random numbers in PyTorch, numpy and\n",
    "    Python.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The desired seed.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJ2ePKogyvCq"
   },
   "outputs": [],
   "source": [
    "# dataset parameters\n",
    "TRAIN_UNITS = [2, 5, 10, 16, 18, 20]\n",
    "TEST_UNITS = [11, 14, 15]\n",
    "\n",
    "DEFAULT_PARAMS = {\n",
    "    # CNN model parameters\n",
    "    'in_channels': 18, \n",
    "    'out_channels': 1,\n",
    "    'window': 50, \n",
    "    'n_ch': 10, \n",
    "    'n_k': 10, \n",
    "    'n_hidden': 50, \n",
    "    'n_layers': 3,\n",
    "    'dropout': 0.,\n",
    "    'padding': 'same',\n",
    "    # training parameters\n",
    "    'batch_size': 256,\n",
    "    'base_lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'max_epochs': 50\n",
    "}\n",
    "\n",
    "DATASETS = create_datasets(df, window_size=DEFAULT_PARAMS['window'], train_units=TRAIN_UNITS, test_units=TEST_UNITS)\n",
    "LOADERS = create_data_loaders(DATASETS, batch_size=DEFAULT_PARAMS['batch_size'], val_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_WqMyBWqyk9U",
    "outputId": "e3f3b51b-8a0b-4cbf-844d-1f4957d62e33"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "model = CNN(\n",
    "  in_channels=DEFAULT_PARAMS['in_channels'],\n",
    "  out_channels=DEFAULT_PARAMS['out_channels'],\n",
    "  n_ch=DEFAULT_PARAMS['n_ch'],\n",
    "  n_k=DEFAULT_PARAMS['n_k'],\n",
    "  n_hidden=DEFAULT_PARAMS['n_hidden'],\n",
    "  n_layers=DEFAULT_PARAMS['n_layers'],\n",
    "  dropout=DEFAULT_PARAMS['dropout'],\n",
    ")\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "  model.parameters(),\n",
    "  lr=DEFAULT_PARAMS['base_lr'],\n",
    "  weight_decay=DEFAULT_PARAMS['weight_decay'],\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "trainer = Trainer(\n",
    "  model,\n",
    "  optimizer,\n",
    "  criterion=criterion,\n",
    "  n_epochs=DEFAULT_PARAMS['max_epochs'],\n",
    "  seed=SEED,\n",
    ")\n",
    "\n",
    "trainer.fit(LOADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "IkwLJMN-sADH",
    "outputId": "1a94ff11-4367-42d6-8263-b4c04200d2d5"
   },
   "outputs": [],
   "source": [
    "trainer.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "fPONTMrK7t2P",
    "outputId": "a0a9ebbb-f5b8-4b27-a91d-150f837d6595"
   },
   "outputs": [],
   "source": [
    "df_test, df_out = trainer.eval_rul_prediction(LOADERS[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "SXyGipjP77TV",
    "outputId": "5298e72a-ab4c-4859-c291-b1f084f4e7ec"
   },
   "outputs": [],
   "source": [
    "df_test.plot(y=['true', 'pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate results of multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single(seed, params=DEFAULT_PARAMS):\n",
    "  seed_everything(seed)\n",
    "\n",
    "  model = CNN(\n",
    "    in_channels=params['in_channels'],\n",
    "    out_channels=params['out_channels'],\n",
    "    n_ch=params['n_ch'],\n",
    "    n_k=params['n_k'],\n",
    "    n_hidden=params['n_hidden'],\n",
    "    n_layers=params['n_layers'],\n",
    "    dropout=params['dropout'],\n",
    "  )\n",
    "\n",
    "  optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=params['base_lr'],\n",
    "    weight_decay=params['weight_decay'],\n",
    "  )\n",
    "\n",
    "  criterion = nn.MSELoss()\n",
    "  trainer = Trainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion=criterion,\n",
    "    n_epochs=params['max_epochs'],\n",
    "    seed=seed,\n",
    "  )\n",
    "\n",
    "  trainer.fit(LOADERS)\n",
    "  df_eval, df_eval_out = trainer.eval_rul_prediction(LOADERS[1])\n",
    "  df_test, df_test_out = trainer.eval_rul_prediction(LOADERS[2])\n",
    "  return df_eval, df_eval_out, df_test, df_test_out\n",
    "\n",
    "N_RUNS = 5\n",
    "df_all = pd.DataFrame()\n",
    "for seed in range(SEED, SEED+N_RUNS):\n",
    "  df_eval, df_eval_out, df_test, df_test_out = run_single(seed)\n",
    "  df_all = df_all.append(df_test_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M3V0Q1rUTU3"
   },
   "source": [
    "## Bonus: Hyperparameter tuning\n",
    "\n",
    "**Model parameters:** These are the parameters that are estimated by the model from the given data. For example the weights of a deep neural network. \n",
    "\n",
    "**Model hyperparameters:** These are the parameters that cannot be estimated by the model from the given data. These parameters are used to estimate the model parameters. For example, the learning rate in deep neural networks.\n",
    "\n",
    "*Hyperparameter tuning* (or hyperparameter optimization) is the process of determining the right combination of hyperparameters that maximizes the model performance. It works by running multiple trials in a single training process. Each trial is a complete execution of your training application with values for your chosen hyperparameters, set within the limits you specify. This process once finished will give you the set of hyperparameter values that are best suited for the model to give optimal results.\n",
    "\n",
    "There are many python libraries for hyperparameter tuning:\n",
    "1. RayTune\n",
    "2. Optuna\n",
    "3. Hyperopt\n",
    "4. sklearn\n",
    "etc.\n",
    "\n",
    "Here we will build a simple tuner using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDsveVb13ka7",
    "outputId": "b2d762f7-a69e-4fd1-8f79-819d1e5a0751"
   },
   "outputs": [],
   "source": [
    "!pip install -U optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLe-fLs58hGh"
   },
   "source": [
    "There are several ways of hyperparameter tuning (all the above mentioned libraries apply several or all of those):\n",
    "\n",
    "GridSearch : We create a discrete search space of hyperparameters to be tuned. This descrete search space grid is exhaustively searched for the best combination.\n",
    "\n",
    "RandomSearch : We define distributions for each hyperparameter. Here the key difference is not all values are tested and values are selected at random. Since randomsearch does not test all hyperparameter values it does not necessarily return the best performing parameters, but it returns a good performing model in significantly shorter time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIOV4QmF7jJg"
   },
   "source": [
    "This is an example of random search, Optuna Tune uses Bayesian optimization by default, however for this time we use the random search algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJ38y2Ev3Aag"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(params):\n",
    "    n_runs = 3 # number of runs to average over, you can decrease this number to speed up the optimization\n",
    "    df_all_val = pd.DataFrame()\n",
    "    for i in n_runs:\n",
    "        seed = SEED + i\n",
    "        df_eval, df_eval_out, df_test, df_test_out = run_single(seed, params)\n",
    "        df_all_val = df_all_val.append(df_eval_out) # append the validation results\n",
    "    # average over n_runs of the validation results, we use the mean of the rmse as the objective to minimize\n",
    "    # we use the validation results for hyperparameter tuning\n",
    "    rmse = df_all_val['rmse'].mean()\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJBN4I6s3AVZ"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # We can add all the parameters that use for defining the model and trainer or even the dataset builder.\n",
    "\n",
    "    # we define the parameter space, you can add more parameters to tune or reduce, you can also start with the best hyperparameters defined in the paper and finetune\n",
    "    params = DEFAULT_PARAMS.copy()\n",
    "    params['base_lr'] = trial.suggest_categorical('base_lr', [0.001, 0.01, 0.1])\n",
    "    params['n_layer'] = trial.suggest_categorical('kernel_size ', [3, 4, 5])\n",
    "    params['kernel_size'] = trial.suggest_categorical('kernel_size ', [3, 5, 7])\n",
    "    ...\n",
    "    \n",
    "    out = evaluate_model(params)\n",
    "    # We need to minimize the predicted rmse.\n",
    "    rmse = out['rmse']\n",
    "    return rmse \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a study name, otherwise a random name will be generated\n",
    "study_name = 'hypertune_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBO1_BFl3Adi",
    "outputId": "956c1b84-2956-449e-bf22-9fe2d9086cd0"
   },
   "outputs": [],
   "source": [
    "n_trials = 2 # number of set of hyperparameters to train\n",
    "# based on the output of the objective, we either maximize or minimize. If we returned accuracy, we would be maximize.    \n",
    "study = optuna.create_study(f'sqlite:///{folder}/study.db',\n",
    "                            study_name=study_name,\n",
    "                            direction=\"minimize\",\n",
    "                            load_if_exists=True,)\n",
    "study.optimize(objective, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c92SU6slDQli"
   },
   "outputs": [],
   "source": [
    "# Load the study for resuming, comment out when reloading\n",
    "# study = optuna.load_study(study_name=study_name, storage=f'sqlite:///{folder}/study.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing impact of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can visualize the importance of hyperparameters\n",
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contour plot between two hyperparameters\n",
    "fig = optuna.visualization.plot_contour(study, params=['lr', 'nl'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can get a dataframe of the hyperparamter results with:\n",
    "hyper_df = study.trials_dataframe()\n",
    "hyper_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the best model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vs-w0nYUAcMB",
    "outputId": "b4677ceb-db36-4f73-8378-09c150d47cff"
   },
   "outputs": [],
   "source": [
    "# Get the est model parameter\n",
    "best_trial = study.best_trial\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"{key}: {value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Prognostics for Turbofine Engines with 1D Convolutional Neural Networks ü©∫\n",
    "\n",
    "## I. Task Description:\n",
    "\n",
    "### Background:\n",
    "Conventional CNNs, primarily developed for image analysis, have shown immense capability in autonomously extracting features from 2D data. This autonomy, achieved through striding or windowing, enables the application of CNNs across various image dimensions without requiring domain expertise. \n",
    "\n",
    "Similarly, 1D CNNs have demonstrated effectiveness in time series data analysis by employing a windowing technique to scan through sequential data, extracting valuable features autonomously. This characteristic of 1D CNNs makes them a powerful tool for prognostics, a field concerned with predicting the Remaining Useful Life (RUL) of systems based on historical and real-time operational data.\n",
    "\n",
    "### Problem Formulation üöÄ:\n",
    "In this task, the objective is to delve into the domain of prognostics to address a real-world problem. Utilizing the architecture described in the paper: \"Fusing Physics-based and Deep Learning Models for Prognostics\" ([link](https://arxiv.org/abs/2003.00732)), you have to develop a 1D CNN model to predict the Remaining Useful Life (RUL) of certain units. The provided dataset comprises time series data from different units, and your goal is to accurately estimate the RUL for specified test units.\n",
    "\n",
    "Training units (engines) **2, 5, 10, 16, 18, 20** \n",
    "\n",
    "Test units (engines) **11, 14, 15** \n",
    "\n",
    "### Exploratory Data Analysis:\n",
    "1. Explore and understand the provided dataset, identifying the key features that could be indicative of a unit's health and consequently its RUL **(10 points max)**.\n",
    "\n",
    "\n",
    "### Implementation:\n",
    "1. Implement the 1D CNN model as per the referenced paper **(20 points max)**. Be careful with the tensor dimensions that you pass to 1DCNN. It should have (n samples, n channels, n timesteps) dimensions.\n",
    "2. Investigate the effect of Dropout and Batch Normalization layers on the model's performance as regularization techniques to mitigate overfitting **(10 points max)**.\n",
    "\n",
    "### Interpretation:\n",
    "1. How do the predictions of RUL evolve over time, within a cycle (i.e a complete flight cycle consisting of taking off, crusing and landing) and between cycles, for a given unit? Visualize and discuss **(30 points max)**.\n",
    "2. How well does your model generalizes to the test units? Identify possible reasons why the model may perform better on some units and worse on others **(20 points max)**. \n",
    "\n",
    "### Evaluation:\n",
    "The performance of your implemented model should be rigorously evaluated across 5 independent runs with different random seeds to ensure the robustness of the results. \n",
    "For each run, utilize the following two common metrics applied in C-MAPSS prognostics analysis:\n",
    "\n",
    "The performance of your implemented model should be evaluated using two common metrics applied in C-MAPSS prognostics analysis:\n",
    "1. **Root Mean Square Error (RMSE)**:\n",
    "\n",
    "$\\text{RMSE} = \\sqrt{\\frac{1}{m^*} \\sum_{j=1}^{m^*} (\\Delta(j))^2}$\n",
    "\n",
    "where $m^*$ denotes the total number of test data samples, and $\\Delta(j)$ is the difference between the estimated and the real Remaining Useful Life (RUL) of the $j$-th sample, i.e., $y(j) - \\hat{y}(j)$.\n",
    "\n",
    "2. **NASA's Scoring Function (s)**:\n",
    "$s = \\sum_{j=1}^{m^*} \\exp\\left(\\alpha \\cdot |\\Delta(j)|\\right) $\n",
    "where $\\alpha = \\frac{1}{13}$ if RUL is underestimated, and $\\alpha = \\frac{1}{10}$ otherwise. The scoring function $s$ is not symmetric and penalizes over-estimation more than under-estimation.\n",
    "\n",
    "\n",
    "Aggregate the results from all **5 runs** to provide a mean and standard deviation for both the RMSE and NASA's Scoring Function. This aggregated evaluation will provide a more robust understanding of the model's performance on the selected prognostics task, comparing it to purely data-driven deep learning models as outlined in the referenced paper.\n",
    "\n",
    "We provide in total **(10 points max)** for proper evaluation of the model. \n",
    "\n",
    "### Bonus:\n",
    "1. Apply hyperparameter tuning with grid search to improve the model performance. You can start with the best hyperparameters from the paper **(10 points max)**. \n",
    "2. Explore alternative architectures or approaches to enhance the prognostic performance further **(10 points max)**.\n",
    "\n",
    "N.B. The maximum of points can not exceed 100 points together with the bonus task.\n",
    "\n",
    "## II. Report Submission:\n",
    "\n",
    "Upon completion of the implementation and evaluation tasks, you are required to submit a comprehensive report summarizing your work and findings. Your report should span 5-8 pages and should be structured as follows:\n",
    "\n",
    "#### Introduction:\n",
    "- A brief introduction/literature review of 1D Convolutional Neural Networks (1DCNN) and their application in time series data analysis, particularly in prognostics.\n",
    "\n",
    "#### Model Description:\n",
    "- A detailed description of the 1DCNN model you implemented, including the architecture, layers, and any regularization techniques used.\n",
    "- Mention of the hyperparameters that were tuned, and any additional modifications made to the original model architecture from the paper.\n",
    "\n",
    "#### Methodology:\n",
    "- Description of the data preparation process, training, and evaluation methodologies.\n",
    "- Mention the training and testing units used as specified in the task.\n",
    "\n",
    "#### Results and Discussion:\n",
    "- Summarization of the evaluation results across 5 runs, providing the mean and standard deviation for both RMSE and NASA's Scoring Function.\n",
    "- Evaluation of the performance per unit, and comparison of your model's performance with the results reported in the referenced paper.\n",
    "- Discussion on the impact of Dropout and Batch Normalization (if applied), and any other observations regarding the model's performance.\n",
    "- Any challenges faced during the implementation and how they were addressed.\n",
    "\n",
    "#### Conclusion:\n",
    "- Summary of key findings, lessons learned, and suggestions for future work to possibly improve the model's performance.\n",
    "\n",
    "#### References:\n",
    "- Proper citation of the paper, any other related works, and resources utilized in completing the assignment.\n",
    "\n",
    "Ensure your report is well-organized, clear, and concise. Visual aids like graphs, tables, and diagrams should be used to enhance the explanation of your work, and comparisons made. The discussion should provide insightful analysis on the model's performance and a critical comparison with the paper's results. The report should be submitted in PDF format by **6.11.2024 Midnight 23:59**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "34888a3cf224db05440f23de7850c4a6b337f0a42ae0f5921fe7f229968509c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
