{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42fd2307-c401-4864-8273-38bcbd5b8b42",
   "metadata": {},
   "source": [
    "# Graded Exercise 1: Extraction of Discriminative Features for Anomaly Detection on Acoustic Data\n",
    "\n",
    "- **Course**: [CIVIL-426 - Machine Learning for Predictive Maintenance](https://edu.epfl.ch/coursebook/en/machine-learning-for-predictive-maintenance-applications-CIVIL-426)\n",
    "- **Start Date**: 2024.09.19 at 10h15\n",
    "- **Due Date**: 2024.10.02 at 23h59\n",
    "- **Student 0**:\n",
    "    - First and Last name: Joshua Cohen-Dumani\n",
    "    - SCIPER: 311105\n",
    "- **Student 1**:\n",
    "    - First and Last name: Sophea Bonne\n",
    "    - SCIPER: 352901\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is divided in 2 parts:\n",
    "- [Part 0](#part-0-demonstration): Demonstration on a toy dataset\n",
    "\n",
    "This part shows how to extract discriminative features for anomaly detection on a toy \n",
    "dataset. This part of the notebook runs without any issues on a properly configured \n",
    "machine. You are **not** expected to fill anything in this part.\n",
    "\n",
    "- [Part 1](#part-1-exercise): Graded exercise\n",
    "\n",
    "This part contains the detailed questions and assignments of the graded exercise. You\n",
    "are expected to fill this part with your own code and your own answers to the questions.\n",
    "\n",
    "⚠️ The deadline to hand-in a PDF report along this complete notebook with \n",
    "[Part 1](#part-1-exercise) filled is **October 2nd, 2024 at 23h59**. No submissions \n",
    "will be accepted past this deadline. ⚠️\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc424a6e-3e0f-45b6-8e12-f9abf739debe",
   "metadata": {},
   "source": [
    "## Part 0: Demonstration\n",
    "\n",
    "### 0. Introduction\n",
    "\n",
    "The **goals** of this exercise are:\n",
    "\n",
    "* Extracting statistical features of acoustic signals to discriminate between normal and abnormal samples\n",
    "* Defining a threshold-based anomaly detection rule based on normal samples only\n",
    "\n",
    "The **requirements** are:\n",
    "* Your method can use a single feature or a combination of features\n",
    "* You must include visualization in a meaningful way\n",
    "* Your method must be developed based on the **TRAIN set** only\n",
    "\n",
    "This part contains an example on a toy dataset.\n",
    "\n",
    "1. **Train**: training set containing only healthy (normal) samples\n",
    "2. **Test**: test set containing both normal and abnormal samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57c54a-fee5-4f02-98e3-dc9eda99688d",
   "metadata": {},
   "source": [
    "### 1. Notebook Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a457d7-efbb-44c5-bfd7-3f456f154cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for visualization\n",
    "# %pip install seaborn matplotlib-venn --upgrade --quiet\n",
    "# Packages for feature extraction and machine learning\n",
    "# %pip install librosa numpy pandas scipy --upgrade --quiet\n",
    "# hello josh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ffca958-becb-48ed-9808-011ce7086841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2, venn3\n",
    "import numpy as np\n",
    "from numpy.fft import fft\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as scist\n",
    "\n",
    "# Matplotlib configuration for notebooks\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a3134-a41c-410b-b964-68c369f0f0f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Load the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230ecb00-8c32-45ab-8197-bd30c3cc8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from your disk (you must have downloaded the file)\n",
    "data = np.load(\"toy_data.npz\")\n",
    "\n",
    "# Extract the training and testing set\n",
    "train_set = data[\"train\"]\n",
    "test_set = data[\"test\"]\n",
    "\n",
    "train_set_size = train_set.shape[0]\n",
    "train_set_timesteps = train_set.shape[1]\n",
    "test_set_size = test_set.shape[0]\n",
    "test_set_timesteps = test_set.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e477c50-f108-4219-a205-d1fbffe06f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About the train dataset\n",
    "print(\n",
    "    f\"The TRAINING set contains {train_set_size} recordings. \"\n",
    "    f\"Each recording contains {train_set_timesteps} samples.\"\n",
    ")\n",
    "\n",
    "# About the test dataset\n",
    "print(\n",
    "    f\"The TESTING set contains {test_set_size} recordings. \"\n",
    "    f\"Each recording contains {test_set_timesteps} samples.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03bc4e-8b21-42e6-9b98-2aa7b2d640fa",
   "metadata": {},
   "source": [
    "### 3. Visual Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707dcccf-8de5-4643-840b-0ff1a1376579",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "ax[0].plot(train_set[0])\n",
    "ax[0].set_title(\"TRAIN set Item 0\")\n",
    "ax[1].plot(test_set[0])\n",
    "ax[1].set_title(\"TEST set Item 0\")\n",
    "ax[2].plot(test_set[1])\n",
    "ax[2].set_title(\"TEST set Item 1\")\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel(\"Sample\")\n",
    "    ax[i].set_ylabel(\"Signal Intensity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bff888-7ec7-4447-8235-74b568b951bf",
   "metadata": {},
   "source": [
    "### 4. Compute Absolute Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c322af6-56a4-483b-a2de-f93fb5832f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_abs = np.abs(train_set)\n",
    "test_set_abs = np.abs(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c91dc-0a21-4f61-8525-d3cc41643803",
   "metadata": {},
   "source": [
    "### 5. Compute Min, Max, Centered Moments and Centralized Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27236bf-4f8b-4dd3-b21a-ee7f638885ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute moments 1 to 4 on the training set and assign them in a Pandas dataframe\n",
    "train_moments = pd.DataFrame(\n",
    "    {\n",
    "        \"MaxAbs\": np.amax(train_set_abs, axis=1),\n",
    "        \"Mean\": np.mean(train_set, axis=1),\n",
    "        \"Variance\": scist.moment(train_set, moment=2, axis=1, nan_policy=\"propagate\"),\n",
    "        \"Skewness\": scist.skew(train_set, axis=1, bias=True, nan_policy=\"propagate\"),\n",
    "        \"Kurtosis\": scist.kurtosis(\n",
    "            train_set, axis=1, fisher=False, bias=True, nan_policy=\"propagate\"\n",
    "        ),\n",
    "        \"type\": \"train\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compute moments 1 to 4 on the testing set and assign them in a Pandas dataframe\n",
    "test_moments = pd.DataFrame(\n",
    "    {\n",
    "        \"MaxAbs\": np.amax(test_set_abs, axis=1),\n",
    "        \"Mean\": np.mean(test_set, axis=1),\n",
    "        \"Variance\": scist.moment(test_set, moment=2, axis=1, nan_policy=\"propagate\"),\n",
    "        \"Skewness\": scist.skew(test_set, axis=1, bias=True, nan_policy=\"propagate\"),\n",
    "        \"Kurtosis\": scist.kurtosis(\n",
    "            test_set, axis=1, fisher=False, bias=True, nan_policy=\"propagate\"\n",
    "        ),\n",
    "        \"type\": \"test\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2319f48-5c33-4323-ae1d-508dd4543e2f",
   "metadata": {},
   "source": [
    "### 6. Fourier Transform and Spectrum Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2dace6a-1ce9-4d61-b858-93f3d08227a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove DC compoent and calculate FFT\n",
    "train_spectrum = 2.0 / train_set_size * np.abs(fft(train_set, axis=1, norm=None))\n",
    "test_spectrum = 2.0 / test_set_size * np.abs(fft(test_set, axis=1, norm=None))\n",
    "\n",
    "# Compute Skewness of the Sprectrum excluding the 0 [Hz] component\n",
    "train_moments[\"SpSkew\"] = scist.skew(\n",
    "    train_spectrum[:, 1:], axis=1, bias=True, nan_policy=\"propagate\"\n",
    ")\n",
    "test_moments[\"SpSkew\"] = scist.skew(\n",
    "    test_spectrum[:, 1:], axis=1, bias=True, nan_policy=\"propagate\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e7ec3-2580-4ec6-8c9b-c23830e52b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working on macOS, you want to specify that the diagonal plot should be histograms to avoid errors when plotting\n",
    "# Otherwise you can leave the lines commented\n",
    "sns.pairplot(\n",
    "    train_moments,\n",
    "    vars=[c for c in train_moments.columns if c != \"type\"],\n",
    "    height=3,\n",
    "    plot_kws={\"alpha\": 0.4, \"s\": 40, \"edgecolor\": None},\n",
    ")\n",
    "#            diag_kind = 'hist',\n",
    "#           diag_kws = {'alpha': 0.4, 'log':True, 'density':True}\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bc176-aa46-451c-8939-07b0befeff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working on macOS, you want to specify that the diagonal plot should be histograms to avoid errors when plotting\n",
    "# Otherwise you can leave the lines commented\n",
    "sns.pairplot(\n",
    "    test_moments,\n",
    "\n",
    "    vars=[c for c in test_moments.columns if c != \"type\"],\n",
    "\n",
    "    height=3,\n",
    "\n",
    "    plot_kws={\"alpha\": 0.4, \"s\": 40, \"edgecolor\": None},\n",
    ")\n",
    "# ,\n",
    "#            diag_kind = 'hist',\n",
    "#           diag_kws = {'alpha': 0.4, 'log':True, 'density':True})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616af22e-b3ac-45e4-9ac1-2f70424bd0a8",
   "metadata": {},
   "source": [
    "### 7. Defining thresholds based on normal data percentiles\n",
    "\n",
    "Here, we define an anomaly detection rule based on thresholds, corresponding to the [0.5, 99.5] percentiles of the feature distributions of normal samples. In other words, a feature higher than 99.5% or lower than 0.5% of the normal features is considered an anomaly. Using only normal samples to base the decision is motivated by the facts that:\n",
    "\n",
    "1. Few anomalies are observed\n",
    "2. We cannot observe every possible anomaly\n",
    "\n",
    "Anomalies are outliers with respect to the normal data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052912c1-4ba9-4f11-8589-52cc132e7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.5, 99.5]\n",
    "limit_max_abs = np.percentile(train_moments.loc[:, \"MaxAbs\"], quantiles)\n",
    "limit_mean = np.percentile(train_moments.loc[:, \"Mean\"], quantiles)\n",
    "limit_variance = np.percentile(train_moments.loc[:, \"Variance\"], quantiles)\n",
    "lim_skew = np.percentile(train_moments.loc[:, \"Skewness\"], quantiles)\n",
    "limit_kurtosis = np.percentile(train_moments.loc[:, \"Kurtosis\"], quantiles)\n",
    "limit_sp_skew = np.percentile(train_moments.loc[:, \"SpSkew\"], quantiles)\n",
    "\n",
    "train_percentage = np.linspace(0, 100, train_set_size)  # From 0% to 100%\n",
    "\n",
    "plt.figure(1, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.1: Percentiles of the 4 first centered Moment for the training set.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(\n",
    "    np.sort(train_moments.loc[:, \"Mean\"]), train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([limit_mean[0], limit_mean[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_mean[1], limit_mean[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Mean-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(\n",
    "    np.sort(train_moments.loc[:, \"Variance\"]), train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([limit_variance[0], limit_variance[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_variance[1], limit_variance[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Variance-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(\n",
    "    np.sort(train_moments.loc[:, \"Skewness\"]), train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([lim_skew[0], lim_skew[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([lim_skew[1], lim_skew[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Skewness-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(\n",
    "    np.sort(train_moments.loc[:, \"Kurtosis\"]), train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([limit_kurtosis[0], limit_kurtosis[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_kurtosis[1], limit_kurtosis[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Kurtosis-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b2bafa-71b3-410d-8270-04969fdf5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\"Fig.2: Percentiles of MaxAbs and SpSkew\", fontsize=20)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"MaxAbs\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_max_abs[0], limit_max_abs[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_max_abs[1], limit_max_abs[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the MaxAbs-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"SpSkew\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_sp_skew[0], limit_sp_skew[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_sp_skew[1], limit_sp_skew[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the SpSkew-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff1dc9b-49c5-4a5e-bcfd-bfcbc24d7d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quantiles = [0.2, 99.8]\n",
    "limit_max_abs = np.percentile(train_moments.loc[:, \"MaxAbs\"], quantiles)\n",
    "limit_mean = np.percentile(train_moments.loc[:, \"Mean\"], quantiles)\n",
    "limit_variance = np.percentile(train_moments.loc[:, \"Variance\"], quantiles)\n",
    "lim_skew = np.percentile(train_moments.loc[:, \"Skewness\"], quantiles)\n",
    "limit_kurtosis = np.percentile(train_moments.loc[:, \"Kurtosis\"], quantiles)\n",
    "limit_sp_skew = np.percentile(train_moments.loc[:, \"SpSkew\"], quantiles)\n",
    "\n",
    "test_percentage = np.linspace(0, 100, test_set_size)  # from 0% to 100% with step 100/nTrain\n",
    "\n",
    "plt.figure(2, figsize=(15, 10))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.3: Percentiles of the 4 first centered moments \"\n",
    "    + \"for both training and test sets with proposed thresholds.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"Mean\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(test_moments.loc[:, \"Mean\"]), test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_mean[0], limit_mean[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_mean[1], limit_mean[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Mean\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"Variance\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(test_moments.loc[:, \"Variance\"]), test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_variance[0], limit_variance[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_variance[1], limit_variance[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"Skewness\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(test_moments.loc[:, \"Skewness\"]), test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([lim_skew[0], lim_skew[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([lim_skew[1], lim_skew[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Skewness\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "# plt.xlim([-50,50])\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"Kurtosis\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(test_moments.loc[:, \"Kurtosis\"]), test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_kurtosis[0], limit_kurtosis[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_kurtosis[1], limit_kurtosis[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Kurtosis\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "# plt.xlim([-50,5000])\n",
    "plt.figlegend(\n",
    "    [\"Train\", \"Test\", \"Threshold\"], loc=\"lower center\", ncol=5, labelspacing=0.0\n",
    ")\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"MaxAbs\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(test_moments.loc[:, \"MaxAbs\"]), test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_max_abs[0], limit_max_abs[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_max_abs[1], limit_max_abs[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"MaxAbs\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"SpSkew\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(test_moments.loc[:, \"SpSkew\"]), test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_sp_skew[0], limit_sp_skew[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_sp_skew[1], limit_sp_skew[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"SpSkew\")\n",
    "plt.ylabel(\"Percentile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec6450-d48a-4d01-88b2-0dc51e9e4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select points either below 0.5 percentile or above 99.5 percentile of each moments and min-max limits\n",
    "abn_train_mean = np.where(\n",
    "    (train_moments.loc[:, \"Mean\"] < limit_mean[0])\n",
    "    | (train_moments.loc[:, \"Mean\"] > limit_mean[1])\n",
    ")[0]\n",
    "abn_train_variance = np.where(\n",
    "    (train_moments.loc[:, \"Variance\"] < limit_variance[0])\n",
    "    | (train_moments.loc[:, \"Variance\"] > limit_variance[1])\n",
    ")[0]\n",
    "abn_train_skew = np.where(\n",
    "    (train_moments.loc[:, \"Skewness\"] < lim_skew[0])\n",
    "    | (train_moments.loc[:, \"Skewness\"] > lim_skew[1])\n",
    ")[0]\n",
    "abn_train_kurtosis = np.where(\n",
    "    (train_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0])\n",
    "    | (train_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1])\n",
    ")[0]\n",
    "\n",
    "abn_train_max_abs = np.where(\n",
    "    (train_moments.loc[:, \"MaxAbs\"] < limit_max_abs[0])\n",
    "    | (train_moments.loc[:, \"MaxAbs\"] > limit_max_abs[1])\n",
    ")[0]\n",
    "abn_train_sp_skew = np.where(\n",
    "    (train_moments.loc[:, \"SpSkew\"] < limit_sp_skew[0])\n",
    "    | (train_moments.loc[:, \"SpSkew\"] > limit_sp_skew[1])\n",
    ")[0]\n",
    "\n",
    "all_abnormalities_train = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            abn_train_mean,\n",
    "            abn_train_variance,\n",
    "            abn_train_skew,\n",
    "            abn_train_kurtosis,\n",
    "            abn_train_max_abs,\n",
    "            abn_train_sp_skew,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compare sets together. Loop over each pair of sets and compute how many IDs they have in common\n",
    "train_dict = {\n",
    "    \"Mean\": abn_train_mean,\n",
    "    \"Variance\": abn_train_variance,\n",
    "    \"Skewness\": abn_train_skew,\n",
    "    \"Kurtsosis\": abn_train_kurtosis,\n",
    "    \"MaxAbs\": abn_train_max_abs,\n",
    "    \"SpSkew\": abn_train_sp_skew,\n",
    "}\n",
    "train_dict_keys = list(train_dict.keys())\n",
    "\n",
    "print(\"TRAINING DATASET\")\n",
    "for i in range(train_dict.__len__()):\n",
    "    k1 = train_dict_keys[i]\n",
    "    print(\"{}: {} anomalies found.\".format(k1, train_dict[k1].__len__()))\n",
    "\n",
    "anomalies_count_train = np.array(\n",
    "    [np.sum([i in train_dict[k] for k in train_dict]) for i in range(train_set_size)]\n",
    ")\n",
    "for i in range(1, 5):\n",
    "    print(\n",
    "        \"{} samples are found as anomalous by at least {} selection methods.\".format(\n",
    "            len(np.where(anomalies_count_train >= i)[0]), i\n",
    "        )\n",
    "    )\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "# select points either below 0.2 percentile or above 99.8 percentile of each moments and min-max limits\n",
    "abn_test_mean = np.where(\n",
    "    (test_moments.loc[:, \"Mean\"] < limit_mean[0])\n",
    "    | (test_moments.loc[:, \"Mean\"] > limit_mean[1])\n",
    ")[0]\n",
    "abn_test_variance = np.where(\n",
    "    (test_moments.loc[:, \"Variance\"] < limit_variance[0])\n",
    "    | (test_moments.loc[:, \"Variance\"] > limit_variance[1])\n",
    ")[0]\n",
    "abn_test_skew = np.where(\n",
    "    (test_moments.loc[:, \"Skewness\"] < lim_skew[0])\n",
    "    | (test_moments.loc[:, \"Skewness\"] > lim_skew[1])\n",
    ")[0]\n",
    "abn_test_kurtosis = np.where(\n",
    "    (test_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0])\n",
    "    | (test_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1])\n",
    ")[0]\n",
    "\n",
    "abn_test_max_abs = np.where(\n",
    "    (test_moments.loc[:, \"MaxAbs\"] < limit_max_abs[0])\n",
    "    | (test_moments.loc[:, \"MaxAbs\"] > limit_max_abs[1])\n",
    ")[0]\n",
    "abn_test_sp_skew = np.where(\n",
    "    (test_moments.loc[:, \"SpSkew\"] < limit_sp_skew[0])\n",
    "    | (test_moments.loc[:, \"SpSkew\"] > limit_sp_skew[1])\n",
    ")[0]\n",
    "\n",
    "allAbnormalities = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            abn_test_mean,\n",
    "            abn_test_variance,\n",
    "            abn_test_skew,\n",
    "            abn_test_kurtosis,\n",
    "            abn_test_max_abs,\n",
    "            abn_test_sp_skew,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# compare sets together. Here I loop over each pair of set and compute how many ID they have in common\n",
    "test_dict = {\n",
    "    \"Mean\": abn_test_mean,\n",
    "    \"Variance\": abn_test_variance,\n",
    "    \"Skewness\": abn_test_skew,\n",
    "    \"Kurtosis\": abn_test_kurtosis,\n",
    "    \"MaxAbs\": abn_test_max_abs,\n",
    "    \"SpSkew\": abn_test_sp_skew,\n",
    "}\n",
    "keysTest = list(test_dict.keys())\n",
    "\n",
    "\n",
    "print(\"\\nTEST DATASET\")\n",
    "for ii in range(test_dict.__len__()):\n",
    "    k1 = keysTest[ii]\n",
    "    print(f\"{k1}: {len(test_dict[k1])} anomalies found.\")\n",
    "\n",
    "countAbnormalTest = np.array(\n",
    "    [np.sum([i in test_dict[k] for k in test_dict]) for i in range(test_set_size)]\n",
    ")\n",
    "for i in range(1, 5):\n",
    "    print(\n",
    "        f\"{len(np.where(countAbnormalTest >= i)[0])} samples are found as anomalous \"\n",
    "        f\"by at least {i} selection methods.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5098d35-3b3c-438e-b35c-58a837f7fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft = np.abs(np.fft.fft(train_set[1]))[: train_set_timesteps // 2]\n",
    "plt.plot(fft, label=\"Normal test sample\")\n",
    "fft = np.abs(np.fft.fft(test_set[0]))[: test_set_timesteps // 2]\n",
    "plt.plot(fft, label=\"Abnormal test sample\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ade3f-7af0-42f2-996d-540041c61659",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.4:Venn diagram between recordings considered abnormals by features 'Mean' and 'Variance'.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "\n",
    "loc_train_mean = (train_moments.loc[:, \"Mean\"] < limit_mean[0]) | (\n",
    "    train_moments.loc[:, \"Mean\"] > limit_mean[1]\n",
    ")\n",
    "loc_train_variance = (train_moments.loc[:, \"Variance\"] < limit_variance[0]) | (\n",
    "    train_moments.loc[:, \"Variance\"] > limit_variance[1]\n",
    ")\n",
    "a1 = np.sum(loc_train_mean & loc_train_variance)\n",
    "a2 = np.sum(loc_train_mean & ~loc_train_variance)\n",
    "a3 = np.sum(~loc_train_mean & loc_train_variance)\n",
    "\n",
    "plt.subplot(121)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Mean\", \"Variance\"))\n",
    "plt.title(\"Training dataset\")\n",
    "\n",
    "\n",
    "loc_test_mean = (test_moments.loc[:, \"Mean\"] < limit_mean[0]) | (\n",
    "    test_moments.loc[:, \"Mean\"] > limit_mean[1]\n",
    ")\n",
    "loc_test_variance = (test_moments.loc[:, \"Variance\"] < limit_variance[0]) | (\n",
    "    test_moments.loc[:, \"Variance\"] > limit_variance[1]\n",
    ")\n",
    "a1 = np.sum(loc_test_mean & loc_test_variance)\n",
    "a2 = np.sum(loc_test_mean & ~loc_test_variance)\n",
    "a3 = np.sum(~loc_test_mean & loc_test_variance)\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Mean\", \"Variance\"))\n",
    "plt.title(\"Test dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0742f-cf56-45ac-9cbc-59f19877a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.5:Venn diagram between recordings considered abnormals by features 'Kurtosis' and 'MaxAbs'.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "\n",
    "loc_train_kurtosis = (train_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0]) | (\n",
    "    train_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1]\n",
    ")\n",
    "loc_train_max_abs = (train_moments.loc[:, \"MaxAbs\"] < limit_max_abs[0]) | (\n",
    "    train_moments.loc[:, \"MaxAbs\"] > limit_max_abs[1]\n",
    ")\n",
    "a1 = np.sum(loc_train_kurtosis & loc_train_max_abs)\n",
    "a2 = np.sum(loc_train_kurtosis & ~loc_train_max_abs)\n",
    "a3 = np.sum(~loc_train_kurtosis & loc_train_max_abs)\n",
    "\n",
    "plt.subplot(121)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Kurtosis\", \"MaxAbs\"))\n",
    "plt.title(\"Training dataset\")\n",
    "\n",
    "\n",
    "loc_test_kurtosis = (test_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0]) | (\n",
    "    test_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1]\n",
    ")\n",
    "loc_test_max_abs = (test_moments.loc[:, \"MaxAbs\"] < limit_max_abs[0]) | (\n",
    "    test_moments.loc[:, \"MaxAbs\"] > limit_max_abs[1]\n",
    ")\n",
    "a1 = np.sum(loc_test_kurtosis & loc_test_max_abs)\n",
    "a2 = np.sum(loc_test_kurtosis & ~loc_test_max_abs)\n",
    "a3 = np.sum(~loc_test_kurtosis & loc_test_max_abs)\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Kurtosis\", \"MaxAbs\"))\n",
    "plt.title(\"Test dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00616f2-f33a-495a-baeb-d48526fe8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.6:Venn diagram between recordings considered abnormals by features 'Kurtosis' and 'SpSkew'.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "\n",
    "loc_train_kurtosis = (train_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0]) | (\n",
    "    train_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1]\n",
    ")\n",
    "loc_train_sp_skew = (train_moments.loc[:, \"SpSkew\"] < limit_sp_skew[0]) | (\n",
    "    train_moments.loc[:, \"SpSkew\"] > limit_sp_skew[1]\n",
    ")\n",
    "a1 = np.sum(loc_train_kurtosis & loc_train_sp_skew)\n",
    "a2 = np.sum(loc_train_kurtosis & ~loc_train_sp_skew)\n",
    "a3 = np.sum(~loc_train_kurtosis & loc_train_sp_skew)\n",
    "\n",
    "plt.subplot(121)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Kurtosis\", \"SpSkew\"))\n",
    "plt.title(\"Training dataset\")\n",
    "\n",
    "\n",
    "loc_test_kurtosis = (test_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0]) | (\n",
    "    test_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1]\n",
    ")\n",
    "loc_test_sp_skew = (test_moments.loc[:, \"SpSkew\"] < limit_sp_skew[0]) | (\n",
    "    test_moments.loc[:, \"SpSkew\"] > limit_sp_skew[1]\n",
    ")\n",
    "a1 = np.sum(loc_test_kurtosis & loc_test_sp_skew)\n",
    "a2 = np.sum(loc_test_kurtosis & ~loc_test_sp_skew)\n",
    "a3 = np.sum(~loc_test_kurtosis & loc_test_sp_skew)\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Kurtosis\", \"SpSkew\"))\n",
    "plt.title(\"Test dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1578a7-f2b8-4d9a-b105-11849837cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_test_kurtosis = (test_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0]) | (\n",
    "    test_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1]\n",
    ")\n",
    "loc_test_mean = (test_moments.loc[:, \"Mean\"] < limit_mean[0]) | (\n",
    "    test_moments.loc[:, \"Mean\"] > limit_mean[1]\n",
    ")\n",
    "loc_test_sp_skew = (test_moments.loc[:, \"SpSkew\"] < limit_sp_skew[0]) | (\n",
    "    test_moments.loc[:, \"SpSkew\"] > limit_sp_skew[1]\n",
    ")\n",
    "\n",
    "a1 = np.sum((loc_test_sp_skew & ~loc_test_kurtosis) & ~loc_test_mean)\n",
    "a2 = np.sum((~loc_test_sp_skew & loc_test_kurtosis) & ~loc_test_mean)\n",
    "a3 = np.sum((loc_test_sp_skew & loc_test_kurtosis) & ~loc_test_mean)\n",
    "a4 = np.sum((~loc_test_sp_skew & ~loc_test_kurtosis) & loc_test_mean)\n",
    "a5 = np.sum((loc_test_sp_skew & ~loc_test_kurtosis) & loc_test_mean)\n",
    "a6 = np.sum((~loc_test_sp_skew & loc_test_kurtosis) & loc_test_mean)\n",
    "a7 = np.sum((loc_test_sp_skew & loc_test_kurtosis) & loc_test_mean)\n",
    "\n",
    "plt.figure(4, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.7:Venn diagram between three features (for the Test dataset).\", fontsize=20\n",
    ")\n",
    "\n",
    "venn3(subsets=(a1, a2, a3, a4, a5, a6, a7), set_labels=(\"SpSkew\", \"Kurtosis\", \"Mean\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb9ade7-6caf-4276-bb61-ac58e8692565",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9986618-73e8-4f44-b002-04973f4b1fa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Exercise\n",
    "\n",
    "⚠️ **This part contains the actual graded exercise 1 questions.** ⚠️\n",
    "\n",
    "Find your own features for a real acoustic dataset from industrial machines!\n",
    "\n",
    "Acoustic emission and vibration monitoring play integral roles in structural health \n",
    "monitoring (SHM) by providing crucial insights into the condition and integrity of \n",
    "structures. Acoustic Emission involves the detection of transient stress waves or \n",
    "acoustic signals emitted by materials when they undergo deformation or damage. Vibration \n",
    "monitoring with accelerometers is widely used to assess the structural integrity of \n",
    "various systems, from bridges and buildings to rotating machinery. Vibration analysis \n",
    "can reveal changes in natural frequencies, mode shapes, and damping characteristics, \n",
    "which can be indicative of structural damage or degradation. \n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "For this exercise, we consider acoustic emissions from two machines: a **pump** and a \n",
    "**valve**. The datasets come as follows:\n",
    "\n",
    "* **pump**\n",
    "    * train set containing data from pump in good working conditions\n",
    "    * test set containing both healthy data and data from pumps with abnormal behaviors\n",
    "* **valve**\n",
    "    * train set containing data from valve in good working conditions\n",
    "    * test set containing both healthy data and data from valves with abnormal behaviors\n",
    "\n",
    "### Problem Description\n",
    "\n",
    "Using features introduced in [Part 0](#part-0-demonstration), other features presented \n",
    "during lectures and previous exercises, or any other features you deem useful, your goal\n",
    "is to identify features that can uncover anomalies present in the test dataset but not \n",
    "present in the training dataset. Selected features will be different for the pump and \n",
    "valve.\n",
    "\n",
    "### Questions\n",
    "\n",
    "**Question 1:** Generate plots of raw signals, FFT spectrums or spectrogram from the \n",
    "healthy data of both the pump and valve acoustic signals. Discuss the distinctions \n",
    "between the signals emitted by these two machines.\n",
    "\n",
    "Questions 2, 3 and 4 have to be answered separately for both the pump and valve datasets.\n",
    "\n",
    "**Question 2:** Visualize the raw signals, spectrum, and spectrograms of the test \n",
    "dataset for the pump/valve dataset. Are there any signals that appear abnormal to you?\n",
    "\n",
    "**Question 3:** Compute basic statistical features (mean, variance, skewness, and \n",
    "kurtosis) for both the training and test datasets of the pump and valve. Are there any \n",
    "abnormal signals that you can detect?\n",
    "\n",
    "**Question 4:** Find by yourself a feature or a combination of feature that help to \n",
    "uncover signals with abnormal behavior. Analyze whether the selected features trigger \n",
    "alarms for similar behavior or if some are specific to particular anomalies.\n",
    "\n",
    "**Question 5:** What are some potential limitations of the method suggested in this \n",
    "exercise for anomaly detection? Answer with at least 3 limitations.\n",
    "\n",
    "**Question 6:** Now that you have developed a set of features and thresholds for a \n",
    "valve/pump, imagine applying them to a different valve/pump. Would the discriminative \n",
    "power change? If so, how do you propose to mitigate it? Justify and provide concrete \n",
    "details.\n",
    "\n",
    "**Question 7:** Now imagine you're in a scenario where the are no anomalous samples \n",
    "available. How would you tackle this problem? Answer with an overview of your proposed \n",
    "approach and then provide concrete details regarding the method.\n",
    "\n",
    "\n",
    "The answers to those questions are expected in a **PDF report**. The full Jupyter notebook \n",
    "must also be submitted. A PDF report without any description, analysis, and discussion \n",
    "around the plots will not be considered valid.\n",
    "\n",
    "### Critical Points & Requirements\n",
    "\n",
    "Here is a unorded non-exhaustive list of some critical points which will be taken into \n",
    "account for grading:\n",
    "- Quality of the plots (legend, axes labels, plot titles, units, ...)\n",
    "- Discriminative power of the selected features (performance) on the `valve` dataset\n",
    "- Discriminative power of the selected features (performance) on the `pump` dataset\n",
    "- Quality of the scientific reasoning\n",
    "- Structure, clarity, and conciseness of the report\n",
    "- Quality of the code (useful but concise documentation, clarity of the code, ...)\n",
    "- ...\n",
    "\n",
    "⚠️ Your submitted notebook should be able to run on a properly configured environment\n",
    "(*c.f.* week 1 exercises). If you require additional packages, make sure to add a \n",
    "`pip install` command in a code cell (example in the following cell to install `gdown`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef3974",
   "metadata": {},
   "source": [
    "### 0. Download the Dataset\n",
    "\n",
    "``gdown`` is a Python library and command-line tool that simplifies the process of downloading files from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d913db99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install gdown --upgrade --quiet\n",
    "import gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pump dataset\n",
    "gdown.download(id=\"1Vz1fhpu5xKJ4RI5maJh_3OweX9BpjBaM\", output=\"./pump.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a096094-91d9-48a7-994d-498050703ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valve dataset\n",
    "gdown.download(id=\"13bzdjL0gEc9hsGHjr0Qo8OMcMF5CrYbS\", output=\"./valve.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be0673",
   "metadata": {},
   "source": [
    "### 1. Extract the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d21dd228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# List all ZIP files in the current directory\n",
    "zip_files = [file for file in os.listdir() if file.endswith(\".zip\")]\n",
    "\n",
    "# Loop through each ZIP file and unzip it in the same directory\n",
    "for zip_file in zip_files:\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "        zip_ref.extractall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5ae7d",
   "metadata": {},
   "source": [
    "### 2. Load the Audio Data\n",
    "\n",
    "In this code cell, we are using the `librosa` library to load audio files from two \n",
    "directories: `/valve` and `/pump`. The goal is to prepare audio data for further \n",
    "analysis or processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22dce0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_files(directory: str) -> np.ndarray:\n",
    "    \"\"\"Load all .WAV audio files in a directory as a 2D Numpy array\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing the audio files\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Loaded audio files as a 2D array\n",
    "    \"\"\"\n",
    "    audio_data: list = []\n",
    "\n",
    "    # Iterate through files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".wav\") or filename.endswith(\".WAV\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            # Load audio file and append it to the list\n",
    "            audio, sr = librosa.load(file_path, sr=None)\n",
    "            audio_data.append(audio)\n",
    "\n",
    "    return np.array(audio_data)\n",
    "\n",
    "\n",
    "# Paths to the \"valve\" and \"pump\" directories\n",
    "valve_directory = \"./valve\"\n",
    "pump_directory = \"./pump\"\n",
    "\n",
    "# Load audio data for \"valve\" and \"pump\"\n",
    "valve_train_data = load_wav_files(os.path.join(valve_directory, \"train\"))\n",
    "valve_test_data = load_wav_files(os.path.join(valve_directory, \"test\"))\n",
    "pump_train_data = load_wav_files(os.path.join(pump_directory, \"train\"))\n",
    "pump_test_data = load_wav_files(os.path.join(pump_directory, \"test\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e68588",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Valve Train Data Shape: {valve_train_data.shape}\")\n",
    "print(f\"Valve Test Data Shape: {valve_test_data.shape}\")\n",
    "print(f\"Pump Train Data Shape: {pump_train_data.shape}\")\n",
    "print(f\"Pump Test Data Shape: {pump_test_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa64f5e",
   "metadata": {},
   "source": [
    "⚠️ **You're expected to fill the notebook from here.** ⚠️\n",
    "\n",
    "### Question 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0321f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all your necessary code to answer question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982682d4",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb3b0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all your necessary code to answer question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d2726",
   "metadata": {},
   "source": [
    "### Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41573645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all your necessary code to answer question 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1ed8a7",
   "metadata": {},
   "source": [
    "### Question 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42ed273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all your necessary code to answer question 4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
